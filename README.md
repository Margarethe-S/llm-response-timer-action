> ‚û°Ô∏è [Click here for the German version](README_de.md)
# üìú llm-response-timer-action

![MIT License](https://img.shields.io/badge/license-MIT-green)
![Tested on LM Studio](https://img.shields.io/badge/tested-LM%20Studio-blue)


This GitHub Action tests a locally running language model via LM Studio by sending a prompt-based request, measuring the response time, providing acoustic feedback upon completion, and saving the results in a log file.


## üîó Used in: Dr. Nature

This GitHub Action was originally developed as part of the [Dr. Nature](https://github.com/Margarethe-S/dr-nature) project ‚Äì a local AI assistant for natural health questions.

‚û°Ô∏è View the main project: **[Dr. Nature ‚Äì A holistic AI-powered health assistant](https://github.com/Margarethe-S/dr-nature)**

## ‚úÖ This project has reached stable core functionality.


The Action has been tested successfully in various local and Docker-based setups. It is now ready for public use and will be continuously improved based on user feedback.


Development remains transparent and learning-focused ‚Äì including all tests, logs, and refinements.


‚úÖ The GitHub Marketplace release is already live.

## ‚òÅÔ∏è Cloud Compatibility (Current Status 16.09.25)
This project was designed and successfully tested for local and Docker-based environments.
Cloud testing (e.g. via AWS EC2) is theoretically possible but requires a locally running LLM (such as Ollama or LM Studio) installed on the cloud server ‚Äì with an open port and a correct API URL provided in the call.
Currently, there is no full web application with an integrated LLM, so direct usage of this Action in the cloud is limited.
Integration into a larger system (e.g. Dr. Nature as a web-based app with an LLM backend) would be a possible future solution for cloud deployments.

## üõ†Ô∏è Features

- Loads the system prompt from a `.txt` file ‚ÑπÔ∏è The file path must be correct depending on your setup (local or Docker).
- Sends a prompt-based request to a locally running LLM (e.g., LM Studio)
- Measures exact response time in seconds and minutes
- Plays a **success tone** when a response is received
- Plays a **warning tone** when a timeout or any error occurs
- Saves the response and timing in a log file
- Differentiates between:
  - ‚è∞ Timeout (after 300 seconds)
  - ‚ùå Connection or request error
  - ‚ùå JSON or key parsing error

## üìÅ Logs

All results are stored in:
`/logs`
After each run, the script creates a `.txt` log file inside the `/logs` folder.

Each log entry includes:
- The used prompt path
- The input question
- The full response or error
- The elapsed time
- Status message (‚Äú‚úÖ Successful‚Äù or error details)

## üß™ Ideal for

- Testing the response time of locally running language models (e.g., LM Studio, oÃ≤llÃ≤aÃ≤mÃ≤aÃ≤)
- Debugging and fine-tuning system prompts
- Automating your local AI development workflow
- Quick functionality checks after prompt changes
- Error tracking with acoustic feedback and log files
- Comparing response times across models or configurations

## üìÇ Example Prompt File


This repository includes a sample prompt file located at:

`prompts/action_prompt1.0.txt`

You can either:
- edit this file directly,
- or provide your own prompt file and pass its path during execution.

‚öôÔ∏è The path is passed as a runtime argument ‚Äì **you don't need to change the code.**

## üõ†Ô∏è Installation

```bash
pip install -r requirements.txt
```

## ‚ñ∂Ô∏è Run the Action Locally

```bash
python main.py http://localhost:1234/v1/chat/completions prompts/action_prompt1.0.txt "Was kann ich gegen Kopfschmerzen auf nat√ºrliche Weise tun?" 
```

üí° Adjust the path syntax for your OS: `\` (Windows) or `/` (macOS/Linux).

> ‚ö†Ô∏è **Pay attention to the exact spelling and spacing in the command!**
>
> - The `localhost` path (`http://localhost:1234/v1/chat/completions`) must **exactly match**  
>   the API URL of your locally running LM Studio instance.
>
> - The three arguments must be separated by spaces:
>   1. The API URL  
>   2. The path to the prompt file (`.txt`)  
>   3. The user question in quotation marks
>
> ‚úÖ You can:
> - Use the provided example prompt file (`prompts/action_prompt1.0.txt`)  
> - Or provide your own prompt file path (e.g. `my_prompts/prompt2.txt`)
>
> üí° If you're unsure which address your LM Studio is listening on,  
> open the settings in LM Studio.  
> You'll find the exact API URL under the menu item **"API"** or **"OpenRouter"**.

The script will:
- Load environment variables and system prompt
- Send a sample user question
- Measure response time
- Print the model‚Äôs answer
- Play system sound
- Log all results in the /logs folder

## üê≥ Run the Action via Docker

```bash
docker run --rm -e TZ=Europe/Berlin -v "C:\path\to\your\project\prompts:/app/prompts" -v "C:\path\to\your\project\logs:/app/logs" llm-response-timer-action http://<YOUR-LOCAL-IP>:1234/v1/chat/completions /app/prompts/action_prompt1.0.txt "Was kann ich gegen Kopfschmerzen auf nat√ºrliche Weise tun?"
```
>üõ†Ô∏è Important notes:
>- Replace "C:\path\to\your\project" with the actual path to your local repository.
>- Replace http://<YOUR-LOCAL-IP>:1234/v1/chat/completions with the real IP of your LM Studio instance.
>- Be precise with spaces and path syntax (Windows vs. Linux/Mac).

Inside the container, the script performs the following steps:
- Loads environment variables and system prompt
- Sends a sample user question to LM Studio
- Measures the response time 

‚ö†Ô∏è Note: Stopwatch may not work reliably inside Docker

- Prints the model‚Äôs answer in the terminal
- Stores the result in the /logs folder (mapped from your local project)

‚ö†Ô∏è Note: Acoustic feedback (system sound) is not available inside Docker

## üñºÔ∏è Screenshots 
Screenshot: Log folder structure

![alt text](images/image-1.png)

Screenshot: Example log file content

![alt text](images/image-3.png)


Screenshot: Terminal response 

![alt text](images/image-4.png)
---
## üîì Use Freely

This repository is shared in the spirit of learning and development.  
Feel free to fork or adapt it if it‚Äôs useful for your own projects.

Good luck with your builds!

üìù License: This project is licensed under the MIT License.  
See the [LICENSE](./LICENSE) file for details.

---

> ‚ö†Ô∏è **Note**  
> This GitHub Action has been successfully tested with a locally running model via **LM Studio** (e.g. *Mistral 7B*).  
> Other models may also work, but have **not been tested yet**.


