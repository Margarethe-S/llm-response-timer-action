> â¡ï¸ [Hier gehtâ€™s zur englischen Version](README.md)
# ğŸ“œ llm-response-timer-action
[![License: AGPL v3](https://img.shields.io/badge/license-AGPL--3.0-green.svg)](https://www.gnu.org/licenses/agpl-3.0)
![Tested on LM Studio](https://img.shields.io/badge/tested-LM%20Studio-blue)


Diese GitHub Action testet ein lokal laufendes Sprachmodell Ã¼ber LM Studio, indem sie eine promptbasierte Anfrage sendet, die Antwortzeit misst, bei Abschluss ein akustisches Feedback ausgibt und die Ergebnisse in einer Log-Datei speichert.


## ğŸ”— Einsatz in Dr. Nature

Diese GitHub Action wurde ursprÃ¼nglich im Rahmen des Projekts [Dr. Nature](https://github.com/Margarethe-S/dr-nature) entwickelt â€“ einer lokalen KI-Anwendung zur Beantwortung naturheilkundlicher Gesundheitsfragen.

â¡ï¸ Zum Hauptprojekt: **[Dr. Nature â€“ A holistic AI-powered health assistant](https://github.com/Margarethe-S/dr-nature)**

## âœ… Dieses Projekt hat einen stabilen Funktionsstand erreicht.


Die Action wurde erfolgreich in lokalen und Docker-basierten Umgebungen getestet. Sie ist jetzt bereit zur Ã¶ffentlichen Nutzung und wird auf Basis von Nutzerfeedback weiterentwickelt.


Die Entwicklung bleibt offen, nachvollziehbar und lernorientiert â€“ inklusive aller Tests, Logs und Optimierungen.

âœ… Die GrundfunktionalitÃ¤t wurde erfolgreich getestet. Eine VerÃ¶ffentlichung im GitHub Marketplace ist erfolgt.

## ğŸ“¡ Cloud-KompatibilitÃ¤t (aktueller Stand 16.09.25)
Das Projekt wurde fÃ¼r lokale und Docker-basierte Umgebungen konzipiert und erfolgreich getestet.
Ein Test in der Cloud (z.â€¯B. via AWS EC2) ist theoretisch mÃ¶glich, erfordert aber ein lokal laufendes LLM (z.â€¯B. Ollama oder LM Studio) auf dem Cloud-Server â€“ inklusive freigegebenem Port und korrekter API-URL im Aufruf.
Momentan existiert keine vollstÃ¤ndige Web-Anwendung mit integrierter LLM, sodass eine direkte Nutzung der Action in der Cloud nur bedingt mÃ¶glich ist.
Die Integration in ein grÃ¶ÃŸeres System (z.â€¯B. Dr. Nature als Web-App mit LLM) wÃ¤re eine mÃ¶gliche LÃ¶sung fÃ¼r zukÃ¼nftige Cloud-Deployments.

## ğŸ› ï¸ Funktionen

- LÃ¤dt den System-Prompt aus einer `.txt`-Datei â„¹ï¸ Der Pfad zur Datei muss je nach Setup (lokal vs. Docker) korrekt angegeben werden.
- Sendet eine promptbasierte Anfrage an ein lokal laufendes Sprachmodell (z.â€¯B. LM Studio)
- Misst die genaue Antwortzeit in Sekunden und Minuten
- Spielt einen **Erfolgston**, wenn eine Antwort empfangen wurde
- Spielt einen **Warnton**, wenn ein Timeout oder Fehler auftritt
- Speichert die Antwort und Zeitmessung in einer Log-Datei
- Unterscheidet zwischen:
  - â° ZeitÃ¼berschreitung (nach 300 Sekunden)
  - âŒ Verbindungs- oder Anfragefehler
  - âŒ JSON- oder SchlÃ¼sselverarbeitungsfehler

## ğŸ“ Logs

Alle Ergebnisse werden gespeichert unter:  
`/logs`

Nach jedem Durchlauf erstellt das Skript eine Log-Datei im Ordner `logs/.txt`.  
Jeder Eintrag enthÃ¤lt:

- Den verwendeten Prompt-Pfad
- Die gestellte Nutzerfrage
- Die vollstÃ¤ndige Antwort oder eine Fehlermeldung
- Die benÃ¶tigte Zeit
- Eine Statusmeldung (â€âœ… Erfolgreichâ€œ oder Details zum Fehler)


## ğŸ§ª Ideal geeignet fÃ¼r

- Testen der Antwortzeit lokal laufender Sprachmodelle (z.â€¯B. LM Studio)
- Debugging und Feintuning von Systemprompts
- Automatisierung deines lokalen KI-Entwicklungsworkflows
- Schnelle Funktionstests bei PromptÃ¤nderungen
- Fehlersuche mit akustischem Feedback und Logdateien
- Vergleich von Reaktionszeiten bei unterschiedlichen Modellen oder Konfigurationen

## ğŸ“‚ Beispiel-Prompt-Datei


In diesem Repository findest du eine Beispiel-Datei unter:

`prompts/action_prompt1.0.txt`

Du kannst entweder:
- diese Datei bearbeiten,
- oder eine eigene Prompt-Datei anlegen und im Aufrufpfad angeben.

ğŸ› ï¸ Der Pfad zur Datei wird beim Aufruf Ã¼bergeben â€“ du musst dafÃ¼r **nichts im Code Ã¤ndern.**


## ğŸ› ï¸ Installation

```bash
pip install -r requirements.txt
```

## â–¶ï¸ Aktion lokal ausfÃ¼hren (ohne Docker)

âœ… Beispiel:
```bash
python main.py http://localhost:1234/v1/chat/completions prompts/action_prompt1.0.txt "Was kann ich gegen Kopfschmerzen auf natÃ¼rliche Weise tun?" 
```
> âš ï¸ **Achte auf die genaue Schreibweise und die Leerzeichen im Befehl!**
> 
> - Der `localhost`-Pfad (`http://localhost:1234/v1/chat/completions`) muss exakt mit der API-URL deines lokal laufenden LM Studios Ã¼bereinstimmen.
> - Die drei Argumente mÃ¼ssen mit Leerzeichen getrennt sein:
>   1. Die API-URL
>   2. Der Pfad zur Prompt-Datei (.txt)
>   3. Die Nutzerfrage in AnfÃ¼hrungszeichen
>
> ğŸ“‚ Du kannst:
> - Die vorhandene Beispiel-Promptdatei verwenden (`prompts/action_prompt1.0.txt`)
> - Oder den Pfad zu deiner eigenen Prompt-Datei angeben (z.â€¯B. `meine_prompts/prompt2.txt`)
>
> ğŸ‘‰ Falls du nicht sicher bist, unter welcher Adresse dein LM Studio erreichbar ist, Ã¶ffne die Einstellungen in LM Studio. Dort findest du die genaue API-URL unter dem MenÃ¼punkt â€APIâ€œ oder â€OpenRouterâ€œ.

Das Skript fÃ¼hrt dann folgende Schritte aus:
- LÃ¤dt Umgebungsvariablen und den Systemprompt
- Sendet eine Beispiel-Nutzerfrage
- Misst die Antwortzeit
- Gibt die Antwort des Modells im Terminal aus
- Spielt einen Systemton ab
- Speichert das Ergebnis im /logs-Ordner

## ğŸ³ Aktion via Docker ausfÃ¼hren

```bash
docker run --rm -e TZ=Europe/Berlin -v "C:\Pfad\zu\deinem\Projekt\prompts:/app/prompts" -v "C:\Pfad\zu\deinem\Projekt\logs:/app/logs" llm-response-timer-action http://<DEINE-LOKALE-IP>:1234/v1/chat/completions /app/prompts/action_prompt1.0.txt "Was kann ich gegen Kopfschmerzen auf natÃ¼rliche Weise tun?"
```
>âš ï¸ Wichtige Hinweise:
>- Ersetze "C:\path\to\your\project" durch den tatsÃ¤chlichen Pfad zu deinem lokalen Projektverzeichnis.
>- Ersetze http://1234/v1/chat/completions durch die korrekte IP-Adresse deines lokal laufenden LM Studio Modells.
>- Achte unbedingt auf die genaue Syntax und Leerzeichen (besonders unter Windows vs. Linux/Mac).

ğŸ’¡ Achte darauf:

>- Der -e TZ=Europe/Berlin Parameter sorgt dafÃ¼r, dass deine Log-Zeiten in der richtigen Zeitzone erscheinen.
>- Die -v Parameter verbinden deine lokalen Ordner mit dem Container:
-prompts â†’ fÃ¼r deine .txt-Promptdateien
-logs â†’ um die Log-Dateien dauerhaft auf deinem PC zu speichern

> Der letzte Teil enthÃ¤lt:
>- die API-URL (z.â€¯B. http://<DEINE-LOKALE-IP>:1234/v1/chat/completions)
>- den Pfad zur Prompt-Datei innerhalb des Containers (/app/prompts/...)
>- und deine Nutzerfrage in AnfÃ¼hrungszeichen

Im Container fÃ¼hrt das Skript folgende Schritte aus:
- LÃ¤dt Umgebungsvariablen und den Systemprompt
- Sendet eine Beispiel-Nutzerfrage an LM Studio
- Misst die Antwortzeit 

âš ï¸ Die Stoppuhr funktioniert in Docker mÃ¶glicherweise nicht zuverlÃ¤ssig

- Gibt die Antwort des Modells im Terminal aus
- Speichert das Ergebnis im /logs-Ordner (gemappt von deinem lokalen Projekt)

âš ï¸ Hinweis: Akustisches Feedback (Systemton) ist innerhalb von Docker nicht verfÃ¼gbar.

---



## ğŸ–¼ï¸ Screenshots 
Screenshot: Log-Ordnerstruktur

![alt text](images/image-1.png)

Screenshot: Beispiel-Inhalt einer Log-Datei

![alt text](images/image.png)

Screenshot: Terminalausgabe der Antwortzeit

![alt text](images/image-2.png)


## ğŸ†“ Frei verwendbar

Dieses Repository wird im Sinne von Lernen und Weiterentwicklung bereitgestellt.  
Du darfst es forken oder anpassen â€“ beachte dabei jedoch die Bedingungen der Lizenz.

ğŸ”’ Bei einer Weitergabe oder Ã¶ffentlichen Nutzung (z.â€¯B. Web-Anwendung, Hosting) bist du verpflichtet, auch deine Ã„nderungen offenzulegen.

ğŸ“œ Lizenz: Dieses Projekt steht unter der GNU Affero General Public License v3.0 (AGPL-3.0).  
Details findest du in der Datei [LICENSE](./LICENSE).

---
 
> âš ï¸ **Hinweis**  
> Diese GitHub Action wurde bisher erfolgreich mit einem lokal laufenden Modell Ã¼ber **LM Studio** getestet (z.â€¯B. *Mistral 7B*).  
> Andere Modelle kÃ¶nnten ebenfalls funktionieren, wurden jedoch **noch nicht getestet**.
