# üìú llm-response-timer-action
> ‚û°Ô∏è [Hier geht‚Äôs zur englischen Version](README.md)

Diese GitHub Action testet ein lokal laufendes Sprachmodell √ºber LM Studio, indem sie eine promptbasierte Anfrage sendet, die Antwortzeit misst und bei Abschluss ein akustisches Feedback ausgibt.

## üîó Einsatz in Dr. Nature

Diese GitHub Action wurde urspr√ºnglich im Rahmen des Projekts [Dr. Nature](https://github.com/Margarethe-Techstarter/dr-nature) entwickelt ‚Äì einer lokalen KI-Anwendung zur Beantwortung naturheilkundlicher Gesundheitsfragen.

‚û°Ô∏è Zum Hauptprojekt: **[Dr. Nature ‚Äì A holistic AI-powered health assistant](https://github.com/Margarethe-Techstarter/dr-nature)**


## ‚úÖ Funktionen

- L√§dt die `.env`-Datei, um die LM Studio API-URL zu erhalten
- L√§dt ein Systemprompt aus einer lokalen `.txt`-Datei
- Sendet einen Prompt per HTTP POST (JSON) an ein lokal laufendes Sprachmodell
- Misst die genaue Antwortzeit in Sekunden und Minuten
- Spielt einen **Erfolgston**, wenn eine Antwort empfangen wurde
- Spielt einen **Warnton**, wenn ein Timeout oder Fehler auftritt
- Unterscheidet zwischen:
  - ‚è∞ Zeit√ºberschreitung (nach 300 Sekunden)
  - ‚ùå Verbindungs- oder Anfragefehler
  - ‚ùå JSON- oder Schl√ºsselverarbeitungsfehler

---

## üìÅ Logs

Alle Ergebnisse werden gespeichert unter:  
`/logs`

Nach jedem Durchlauf erstellt das Skript eine Log-Datei im Ordner `logs/.txt`.  
Jeder Eintrag enth√§lt:

- Den verwendeten Prompt-Pfad
- Die gestellte Nutzerfrage
- Die vollst√§ndige Antwort oder eine Fehlermeldung
- Die ben√∂tigte Zeit
- Eine Statusmeldung (‚Äû‚úÖ Erfolgreich‚Äú oder Details zum Fehler)


## üß™ Ideal geeignet f√ºr

- Testen der Antwortzeit lokal laufender Sprachmodelle
- Debugging von Systemprompts
- Automatisierung deines KI-Entwicklungs-Workflows


## üìÇ Beispiel-Prompt-Datei

Speichere deinen Prompt zum Beispiel in:
`prompts/action_promt1.0.txt`

Du kannst den Pfad in der `main.py` anpassen ‚Äì direkt √ºber die Variable `prompt_path`.


## ‚öôÔ∏è Voraussetzungen

Erstelle eine `.env`-Datei im Hauptverzeichnis deines Projekts mit folgendem Inhalt:

```env
LMSTUDIO_API_URL=http://localhost:1234/v1/chat/completions
```

## üõ†Ô∏è Installation

```bash
pip install -r requirements.txt
```

## ‚ñ∂Ô∏è Aktion lokal ausf√ºhren

```bash
python main.py
```
Das Skript f√ºhrt dann folgende Schritte aus:
- L√§dt Umgebungsvariablen und den Systemprompt
- Sendet eine Beispiel-Nutzerfrage
- Misst die Antwortzeit
- Gibt die Antwort des Modells im Terminal aus
- Spielt einen Systemton ab
- Speichert das Ergebnis im /logs-Ordner


## üñºÔ∏è Screenshots 
Screenshot: Log-Ordnerstrucktur

![alt text](images/image-1.png)

Screenshot: Beispiel-Inhalt einer Log-Datei

![alt text](images/image.png)

Screenshot: Terminalausgabe der Antwortzeit

![alt text](images/image-2.png)

---
## üîì Frei verwendbar

Dieses Repository wird im Sinne von Lernen und Weiterentwicklung bereitgestellt.  
Gerne darfst du es forken oder anpassen, wenn es dir bei eigenen Projekten hilft.

Viel Erfolg bei deinen Entwicklungen!

---
 
> ‚ö†Ô∏è **Hinweis**  
> Diese GitHub Action wurde bisher erfolgreich mit einem lokal laufenden Modell √ºber **LM Studio** getestet (z.‚ÄØB. *Mistral 7B*).  
> Andere Modelle k√∂nnten ebenfalls funktionieren, wurden jedoch **noch nicht getestet**.
