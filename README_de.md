> â¡ï¸ [Hier gehtâ€™s zur englischen Version](README.md)
# ğŸ“œ llm-response-timer-action
![MIT License](https://img.shields.io/badge/license-MIT-green)
![Tested on LM Studio](https://img.shields.io/badge/tested-LM%20Studio-blue)


Diese GitHub Action testet ein lokal laufendes Sprachmodell Ã¼ber LM Studio, indem sie eine promptbasierte Anfrage sendet, die Antwortzeit misst, bei Abschluss ein akustisches Feedback ausgibt und die Ergebnisse in einer Log-Datei speichert.


## ğŸ”— Einsatz in Dr. Nature

Diese GitHub Action wurde ursprÃ¼nglich im Rahmen des Projekts [Dr. Nature](https://github.com/Margarethe-S/dr-nature) entwickelt â€“ einer lokalen KI-Anwendung zur Beantwortung naturheilkundlicher Gesundheitsfragen.

â¡ï¸ Zum Hauptprojekt: **[Dr. Nature â€“ A holistic AI-powered health assistant](https://github.com/Margarethe-S/dr-nature)**

## âœ… Dieses Projekt hat einen stabilen Funktionsstand erreicht.


Die Action wurde erfolgreich in lokalen und Docker-basierten Umgebungen getestet. Sie ist jetzt bereit zur Ã¶ffentlichen Nutzung und wird auf Basis von Nutzerfeedback weiterentwickelt.


Die Entwicklung bleibt offen, nachvollziehbar und lernorientiert â€“ inklusive aller Tests, Logs und Optimierungen.

âœ… Die GrundfunktionalitÃ¤t wurde erfolgreich getestet. Eine VerÃ¶ffentlichung im GitHub Marketplace ist erfolgt.


## ğŸ› ï¸ Funktionen

- LÃ¤dt den System-Prompt aus einer `.txt`-Datei â„¹ï¸ Der Pfad zur Datei muss je nach Setup (lokal vs. Docker) korrekt angegeben werden.
- Sendet eine promptbasierte Anfrage an ein lokal laufendes Sprachmodell (z.â€¯B. LM Studio)
- Misst die genaue Antwortzeit in Sekunden und Minuten
- Spielt einen **Erfolgston**, wenn eine Antwort empfangen wurde
- Spielt einen **Warnton**, wenn ein Timeout oder Fehler auftritt
- Speichert die Antwort und Zeitmessung in einer Log-Datei
- Unterscheidet zwischen:
  - â° ZeitÃ¼berschreitung (nach 300 Sekunden)
  - âŒ Verbindungs- oder Anfragefehler
  - âŒ JSON- oder SchlÃ¼sselverarbeitungsfehler

## ğŸ“ Logs

Alle Ergebnisse werden gespeichert unter:  
`/logs`

Nach jedem Durchlauf erstellt das Skript eine Log-Datei im Ordner `logs/.txt`.  
Jeder Eintrag enthÃ¤lt:

- Den verwendeten Prompt-Pfad
- Die gestellte Nutzerfrage
- Die vollstÃ¤ndige Antwort oder eine Fehlermeldung
- Die benÃ¶tigte Zeit
- Eine Statusmeldung (â€âœ… Erfolgreichâ€œ oder Details zum Fehler)


## ğŸ§ª Ideal geeignet fÃ¼r

- Testen der Antwortzeit lokal laufender Sprachmodelle (z.â€¯B. LM Studio)
- Debugging und Feintuning von Systemprompts
- Automatisierung deines lokalen KI-Entwicklungsworkflows
- Schnelle Funktionstests bei PromptÃ¤nderungen
- Fehlersuche mit akustischem Feedback und Logdateien
- Vergleich von Reaktionszeiten bei unterschiedlichen Modellen oder Konfigurationen

## ğŸ“‚ Beispiel-Prompt-Datei


In diesem Repository findest du eine Beispiel-Datei unter:

`prompts/action_prompt1.0.txt`

Du kannst entweder:
- diese Datei bearbeiten,
- oder eine eigene Prompt-Datei anlegen und im Aufrufpfad angeben.

ğŸ› ï¸ Der Pfad zur Datei wird beim Aufruf Ã¼bergeben â€“ du musst dafÃ¼r **nichts im Code Ã¤ndern.**


## ğŸ› ï¸ Installation

```bash
pip install -r requirements.txt
```

## â–¶ï¸ Aktion lokal ausfÃ¼hren (ohne Docker)

âœ… Beispiel:
```bash
python actions/timer/main.py http://localhost:1234/v1/chat/completions prompts/action_prompt1.0.txt "Was kann ich gegen Kopfschmerzen auf natÃ¼rliche Weise tun?" 
```
> âš ï¸ **Achte auf die genaue Schreibweise und die Leerzeichen im Befehl!**
> 
> - Der `localhost`-Pfad (`http://localhost:1234/v1/chat/completions`) muss exakt mit der API-URL deines lokal laufenden LM Studios Ã¼bereinstimmen.
> - Die drei Argumente mÃ¼ssen mit Leerzeichen getrennt sein:
>   1. Die API-URL
>   2. Der Pfad zur Prompt-Datei (.txt)
>   3. Die Nutzerfrage in AnfÃ¼hrungszeichen
>
> ğŸ“‚ Du kannst:
> - Die vorhandene Beispiel-Promptdatei verwenden (`prompts/action_prompt1.0.txt`)
> - Oder den Pfad zu deiner eigenen Prompt-Datei angeben (z.â€¯B. `meine_prompts/prompt2.txt`)
>
> ğŸ‘‰ Falls du nicht sicher bist, unter welcher Adresse dein LM Studio erreichbar ist, Ã¶ffne die Einstellungen in LM Studio. Dort findest du die genaue API-URL unter dem MenÃ¼punkt â€APIâ€œ oder â€OpenRouterâ€œ.

Das Skript fÃ¼hrt dann folgende Schritte aus:
- LÃ¤dt Umgebungsvariablen und den Systemprompt
- Sendet eine Beispiel-Nutzerfrage
- Misst die Antwortzeit
- Gibt die Antwort des Modells im Terminal aus
- Spielt einen Systemton ab
- Speichert das Ergebnis im /logs-Ordner

## ğŸ³ Aktion via Docker ausfÃ¼hren

```bash
docker run --rm -e TZ=Europe/Berlin -v "C:\Pfad\zu\deinem\Projekt\prompts:/app/prompts" -v "C:\Pfad\zu\deinem\Projekt\logs:/app/logs" llm-response-timer-action http://<DEINE-LOKALE-IP>:1234/v1/chat/completions /app/prompts/action_prompt1.0.txt "Was kann ich gegen Kopfschmerzen auf natÃ¼rliche Weise tun?"
```
>âš ï¸ Wichtige Hinweise:
>- Ersetze "C:\path\to\your\project" durch den tatsÃ¤chlichen Pfad zu deinem lokalen Projektverzeichnis.
>- Ersetze http://1234/v1/chat/completions durch die korrekte IP-Adresse deines lokal laufenden LM Studio Modells.
>- Achte unbedingt auf die genaue Syntax und Leerzeichen (besonders unter Windows vs. Linux/Mac).

ğŸ’¡ Achte darauf:

>- Der -e TZ=Europe/Berlin Parameter sorgt dafÃ¼r, dass deine Log-Zeiten in der richtigen Zeitzone erscheinen.
>- Die -v Parameter verbinden deine lokalen Ordner mit dem Container:
-prompts â†’ fÃ¼r deine .txt-Promptdateien
-logs â†’ um die Log-Dateien dauerhaft auf deinem PC zu speichern

> Der letzte Teil enthÃ¤lt:
>- die API-URL (z.â€¯B. http://<DEINE-LOKALE-IP>:1234/v1/chat/completions)
>- den Pfad zur Prompt-Datei innerhalb des Containers (/app/prompts/...)
>- und deine Nutzerfrage in AnfÃ¼hrungszeichen

Im Container fÃ¼hrt das Skript folgende Schritte aus:
- LÃ¤dt Umgebungsvariablen und den Systemprompt
- Sendet eine Beispiel-Nutzerfrage an LM Studio
- Misst die Antwortzeit 

âš ï¸ Die Stoppuhr funktioniert in Docker mÃ¶glicherweise nicht zuverlÃ¤ssig

- Gibt die Antwort des Modells im Terminal aus
- Speichert das Ergebnis im /logs-Ordner (gemappt von deinem lokalen Projekt)

âš ï¸ Hinweis: Akustisches Feedback (Systemton) ist innerhalb von Docker nicht verfÃ¼gbar.

---



## ğŸ–¼ï¸ Screenshots 
Screenshot: Log-Ordnerstrucktur

![alt text](images/image-1.png)

Screenshot: Beispiel-Inhalt einer Log-Datei

![alt text](images/image.png)

Screenshot: Terminalausgabe der Antwortzeit

![alt text](images/image-2.png)

---
## ğŸ”“ Frei verwendbar

Dieses Repository wird im Sinne von Lernen und Weiterentwicklung bereitgestellt.  
Gerne darfst du es forken oder anpassen, wenn es dir bei eigenen Projekten hilft.

Viel Erfolg bei deinen Entwicklungen!

ğŸ“ Lizenz: Dieses Projekt steht unter der MIT-Lizenz.  
Details findest du in der Datei [LICENSE](./LICENSE).

---
 
> âš ï¸ **Hinweis**  
> Diese GitHub Action wurde bisher erfolgreich mit einem lokal laufenden Modell Ã¼ber **LM Studio** getestet (z.â€¯B. *Mistral 7B*).  
> Andere Modelle kÃ¶nnten ebenfalls funktionieren, wurden jedoch **noch nicht getestet**.
